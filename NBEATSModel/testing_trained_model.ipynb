{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lstm-example/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import NBEATSModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model: Invalid magic number; corrupt file?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lstm-example/lib/python3.10/site-packages/darts/models/forecasting/torch_forecasting_model.py:1747: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model: TorchForecastingModel = torch.load(\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "data_path = 'prepared_data_for_prophet.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert 'ds' column to datetime\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# Load the model using Darts\n",
    "try:\n",
    "    nbeats_model = NBEATSModel.load('/Users/ammarahmad/Documents/Its IT Group/Fuel Price TimeSeries/lstm-example/NBEATSModel/trained_nbeats_model.pkl')\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load model:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_NaN(data):\n",
    "    # Select only numeric columns for spline interpolation\n",
    "    numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    print(\"Number of NaN values:\\n\", data[numeric_columns].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'ds' column is in datetime format\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# Define the start and end date for the prediction\n",
    "start_date = datetime(2024, 7, 1, 0, 0)\n",
    "end_date = start_date + timedelta(weeks=3)  # 3 weeks\n",
    "\n",
    "# Define the period for the past data (3 months before the start date)\n",
    "past_start_date = start_date - timedelta(weeks=12)  # 3 months ~ 12 weeks\n",
    "\n",
    "# Filter the past data (past 3 months)\n",
    "past_data = df[(df['ds'] >= past_start_date) & (df['ds'] < start_date)]\n",
    "\n",
    "# Extract target series ('y') and regressors\n",
    "regressors = [col for col in df.columns if col not in ['y', 'ds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert past data to TimeSeries format for Darts\n",
    "series = TimeSeries.from_dataframe(past_data, 'ds', 'y')\n",
    "covariates = TimeSeries.from_dataframe(past_data, 'ds', regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler_series = Scaler()\n",
    "scaler_covariates = Scaler()\n",
    "\n",
    "series = scaler_series.fit_transform(series)\n",
    "covariates = scaler_covariates.fit_transform(covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_predict_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use the model to predict future values\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m predicted_series \u001b[38;5;241m=\u001b[39m \u001b[43mnbeats_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcovariates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert predicted time series back to a DataFrame for easier visualization and comparison\u001b[39;00m\n\u001b[1;32m      8\u001b[0m predicted_df \u001b[38;5;241m=\u001b[39m predicted_series\u001b[38;5;241m.\u001b[39mpd_dataframe()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lstm-example/lib/python3.10/site-packages/darts/utils/torch.py:103\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[1;32m    102\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lstm-example/lib/python3.10/site-packages/darts/models/forecasting/torch_forecasting_model.py:1465\u001b[0m, in \u001b[0;36mTorchForecastingModel.predict\u001b[0;34m(self, n, series, past_covariates, future_covariates, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters, show_warnings)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m   1447\u001b[0m     n,\n\u001b[1;32m   1448\u001b[0m     series,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     show_warnings\u001b[38;5;241m=\u001b[39mshow_warnings,\n\u001b[1;32m   1454\u001b[0m )\n\u001b[1;32m   1456\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_inference_dataset(\n\u001b[1;32m   1457\u001b[0m     target\u001b[38;5;241m=\u001b[39mseries,\n\u001b[1;32m   1458\u001b[0m     n\u001b[38;5;241m=\u001b[39mn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1463\u001b[0m )\n\u001b[0;32m-> 1465\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroll_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroll_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmc_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmc_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_likelihood_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_likelihood_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m called_with_single_series \u001b[38;5;28;01melse\u001b[39;00m predictions\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lstm-example/lib/python3.10/site-packages/darts/utils/torch.py:103\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[1;32m    102\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lstm-example/lib/python3.10/site-packages/darts/models/forecasting/torch_forecasting_model.py:1583\u001b[0m, in \u001b[0;36mTorchForecastingModel.predict_from_dataset\u001b[0;34m(self, n, input_series_dataset, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters)\u001b[0m\n\u001b[1;32m   1580\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;66;03m# set prediction parameters\u001b[39;00m\n\u001b[0;32m-> 1583\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_predict_parameters\u001b[49m(\n\u001b[1;32m   1584\u001b[0m     n\u001b[38;5;241m=\u001b[39mn,\n\u001b[1;32m   1585\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39mnum_samples,\n\u001b[1;32m   1586\u001b[0m     roll_size\u001b[38;5;241m=\u001b[39mroll_size,\n\u001b[1;32m   1587\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1588\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m   1589\u001b[0m     predict_likelihood_parameters\u001b[38;5;241m=\u001b[39mpredict_likelihood_parameters,\n\u001b[1;32m   1590\u001b[0m     mc_dropout\u001b[38;5;241m=\u001b[39mmc_dropout,\n\u001b[1;32m   1591\u001b[0m )\n\u001b[1;32m   1593\u001b[0m dataloader_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1594\u001b[0m     {\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[1;32m   1602\u001b[0m )\n\u001b[1;32m   1604\u001b[0m pred_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m   1605\u001b[0m     input_series_dataset,\n\u001b[1;32m   1606\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataloader_kwargs,\n\u001b[1;32m   1607\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_predict_parameters'"
     ]
    }
   ],
   "source": [
    "# Number of data points to predict (3 weeks of hourly data)\n",
    "n = 24 * 7 * 3\n",
    "\n",
    "# Use the model to predict future values\n",
    "predicted_series = nbeats_model.predict(n=n, series=series, past_covariates=covariates)\n",
    "\n",
    "# Convert predicted time series back to a DataFrame for easier visualization and comparison\n",
    "predicted_df = predicted_series.pd_dataframe()\n",
    "predicted_df.columns = ['predicted']\n",
    "\n",
    "# Print the first few rows of the prediction results\n",
    "print(predicted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
